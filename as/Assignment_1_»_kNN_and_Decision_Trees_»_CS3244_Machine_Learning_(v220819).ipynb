{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hjunleon/CS3244/blob/master/Assignment_1_%C2%BB_kNN_and_Decision_Trees_%C2%BB_CS3244_Machine_Learning_(v220819).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a855393",
      "metadata": {
        "id": "9a855393"
      },
      "source": [
        "Available at http://www.comp.nus.edu.sg/~cs3244/AY22S1/01.assignment.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbbae3e4-d223-47a6-9825-f548841c5a57",
      "metadata": {
        "id": "fbbae3e4-d223-47a6-9825-f548841c5a57"
      },
      "source": [
        "![Machine Learning](https://www.comp.nus.edu.sg/~cs3244/AY22S1/2210-header.png)\n",
        "---\n",
        "<!-- See **Credits** below for acknowledgements and rights.  For NUS class credit, you'll need to do the corresponding _Assessment_ in [CS3244 in Coursemology](http://coursemology.org/courses/1870) by the respective deadline (mentioned at the end of the notebook).  -->\n",
        "\n",
        "**You must acknowledge that your submitted assignment in LumiNUS is your independent work and that you followed course policy.  This assignment is due on 4 Sep 2022 (Sun) at 23:59 pm (i.e., before Monday).  Make sure you reserve sufficient time to fill in the corresponding LumiNUS Quiz and uploading your exported copy to LumiNUS Files.**\n",
        "\n",
        "**You must acknowledge that your submitted assessment is your independent work.**\n",
        "\n",
        "**<font color='green'>This version is the current version of this file – version 220819 (19 Aug 2022).  Only parts of the section on kNN have been changed; we have removed all references to cosine similarity (another useful distance metric, but beyond the scope of what we will cover here in CS3244 Machine Learning.</font>**\n",
        "\n",
        "**<font color='green'>If you attempted the earlier v220811 version, only answers to Questions 1–4 have changed. </font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TZIaecT9uF6r",
      "metadata": {
        "id": "TZIaecT9uF6r"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "We have introduced $k$**-Nearest Neighbors** and **Decision Trees** in the lectures on Weeks 02 and 03. In this **individual** assignment, you will be graded on implementing the models yourself. You will run both models and compare their performance on the same dataset. Overall, you will learn to appreciate the similarities and differences in models to predict on the same problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hFe14MROoltp",
      "metadata": {
        "id": "hFe14MROoltp"
      },
      "source": [
        "## Assignment Instructions \n",
        "\n",
        "Before the assignment, you should create a copy of this Colab file in your own Google Drive. \n",
        "\n",
        "In this assignment, you will\n",
        "\n",
        "1. Follow the instructions to familiarize yourself with the `wine` dataset;\n",
        "2. Write your code in the designated spaces to finish the implementations of the $k$-Nearest Neighbor and Decision Tree algorithms;\n",
        "2. Run the notebook to obtain your results;\n",
        "3. Copy each of your code and result, and  paste into the Luminus quiz submission.\n",
        "4. Upload this Colab file into Luminus \"Files/Assignment#1 Submissions\n",
        "\n",
        "**Assignment Deadline: September 4th (Sunday) 23:59 SGT**  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75b5185-8cb4-49c3-978f-7c234bee90b8",
      "metadata": {
        "id": "a75b5185-8cb4-49c3-978f-7c234bee90b8"
      },
      "source": [
        "## 1. Programming: $k$-NN and Decision Tree from `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e5f97a-beff-4773-b90c-55ec53eec6e0",
      "metadata": {
        "id": "24e5f97a-beff-4773-b90c-55ec53eec6e0"
      },
      "source": [
        "### .a Loading and Visualizing Input data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fcb9eb-aa7d-43db-9378-8af8983eec88",
      "metadata": {
        "id": "59fcb9eb-aa7d-43db-9378-8af8983eec88"
      },
      "source": [
        "\n",
        "We'll use the [Wine](https://archive.ics.uci.edu/ml/datasets/Wine) dataset from the popular [UCI dataset repository](https://archive.ics.uci.edu/ml/index.php).  This describes the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 chemical constituents found in each of the three types of wines.\n",
        "\n",
        "We'll load in the data for white wines, take a look around and split the data into parts for training the model and testing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebed0d46-50bd-447d-9292-256bfacab8cd",
      "metadata": {
        "id": "ebed0d46-50bd-447d-9292-256bfacab8cd"
      },
      "outputs": [],
      "source": [
        "# Import the standard tools for pythonic data analysis.\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "# Let's read the data in as a \"data frame\" (df), equivalent to our D = (X,y) data matrix\n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv',sep=';') # Separate on semicolons"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6af47084-cf5c-44f7-9c7a-dc7b59b0decb",
      "metadata": {
        "id": "6af47084-cf5c-44f7-9c7a-dc7b59b0decb"
      },
      "source": [
        "We want to apply matchine learning algorithms to predict the quality of the wine from its constituents. That is, we will utilize algorithms that can find the correlation between wine quality and its 13 constituent (as features) to make prediction. To better understand how algorithms can achieve this, we need to inspect the data distribution first.\n",
        "<!-- (actually, we can predict any feature from any other feature, there's really no particular distinction between $\\mathbf{x}$ and $y$).   -->\n",
        "Let's take a look at the distribution for **quality**. This tells us the how the wine quality is distributed without knowing anything about other information. Go ahead and run the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "778831d9-de53-4da4-8ade-e3dcbd2922b8",
      "metadata": {
        "id": "778831d9-de53-4da4-8ade-e3dcbd2922b8"
      },
      "outputs": [],
      "source": [
        "# Get numeric distributions of our quality output\n",
        "print(\"Number of wines of a particular rating:\")\n",
        "counts = df['quality'].value_counts()\n",
        "print(counts)\n",
        "\n",
        "# Let's do a histogram plot. To do that we need to specify the \n",
        "# y-axis that we want to plot – i.e. 'quality' – and number of bins\n",
        "df.hist(column = 'quality', bins = len(counts))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1904d12-8911-4ea4-a4dc-5cb3157a0fd1",
      "metadata": {
        "id": "c1904d12-8911-4ea4-a4dc-5cb3157a0fd1"
      },
      "source": [
        "This looks somewhat normally distributed.  Let's say we care about good wines.  Then we'll just concentrate on differentiating great wines from the rest. Our task is now a classification. As we can see from the histogram below, the two classes are heavily imbalanced -- we have far more \"Not Good\" wines than the \"Good\" wines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aebf57c-d97c-469b-9bd5-68f660902706",
      "metadata": {
        "id": "9aebf57c-d97c-469b-9bd5-68f660902706"
      },
      "outputs": [],
      "source": [
        "# Create a new column called good wine, where the value of the quality is 7 or better.\n",
        "df['good wine'] = np.where(df['quality']>=7, \"Good\", \"Not Good\")\n",
        "\n",
        "# Then remove the quality column (why)\n",
        "df.drop('quality', axis=1, inplace=True)\n",
        "\n",
        "df['good wine'].value_counts().plot(kind = 'bar', title = 'Distribution of classes')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e34552f-c406-440a-a3fa-86e9337e7013",
      "metadata": {
        "id": "2e34552f-c406-440a-a3fa-86e9337e7013"
      },
      "source": [
        "### .b Split dataset into train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd2e9a7f-21ce-461a-884e-519bfb045f8a",
      "metadata": {
        "id": "fd2e9a7f-21ce-461a-884e-519bfb045f8a"
      },
      "source": [
        "Let's split the data into training and testing data sets before we do anything else.   It's important to look only at the training data to develop our intuitions (**why?**) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8492c0-aea9-44ce-8f74-38bd2f24b038",
      "metadata": {
        "id": "1d8492c0-aea9-44ce-8f74-38bd2f24b038"
      },
      "outputs": [],
      "source": [
        "# Learn how to split test and training data from a whole\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Partion the features from the class to predict\n",
        "df_X = df[df.columns[df.columns != 'good wine']].copy() # get columns that are not 'good wine'\n",
        "df_y = df['good wine'].copy() # get the column named 'good wine'; this is our label\n",
        "\n",
        "# (random_state): we use a fixed random seed so we get the same results every time.\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.3, random_state=1)\n",
        "\n",
        "print (\"Number of training instances: \", len(X_train), \"\\nNumber of test instances: \", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b988fbc-d3ec-492b-b314-46a8306d2040",
      "metadata": {
        "id": "0b988fbc-d3ec-492b-b314-46a8306d2040"
      },
      "source": [
        "Let's look at the first few rows of the training portion of the dataset.  Understanding the data is **always** important in trying to build any model for prediction.\n",
        "You can check against the description of the dataset which is here: https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc01a3e1-8fc8-4bec-b574-8bfbc930ffbc",
      "metadata": {
        "id": "fc01a3e1-8fc8-4bec-b574-8bfbc930ffbc"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4d9054-cb06-47e1-bf2c-22b3421a1485",
      "metadata": {
        "id": "5a4d9054-cb06-47e1-bf2c-22b3421a1485"
      },
      "source": [
        "We can also take a look at some general statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f10b76-0307-4e5b-9cd0-b4b2f05ef55c",
      "metadata": {
        "id": "72f10b76-0307-4e5b-9cd0-b4b2f05ef55c"
      },
      "outputs": [],
      "source": [
        "X_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad0e0a49-c2a0-490d-970a-71eeebb6ed58",
      "metadata": {
        "id": "ad0e0a49-c2a0-490d-970a-71eeebb6ed58"
      },
      "source": [
        "### .c Train and Test the models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbc9decb-8422-4740-8301-ca2bec8c4b06",
      "metadata": {
        "id": "cbc9decb-8422-4740-8301-ca2bec8c4b06"
      },
      "source": [
        "Actually all of the above was just preparation. Now we have some intution, ;let's try fitting the dataset on both $k$ - NN and Decision Tree models using sklearn's API library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98efb8fa-3bca-4734-8f71-d6f37ddde246",
      "metadata": {
        "id": "98efb8fa-3bca-4734-8f71-d6f37ddde246"
      },
      "outputs": [],
      "source": [
        "# Get the machine learning algorithm k-NN\n",
        "from sklearn import neighbors\n",
        "\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors = 1, metric='euclidean')\n",
        "knn_model = knn.fit(X_train, y_train)\n",
        "print('kNN accuracy for training set: %f' % knn_model.score(X_train, y_train))\n",
        "print('kNN accuracy for test set: %f' % knn_model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aa8375a-a076-49a9-b182-d43bb1247288",
      "metadata": {
        "id": "2aa8375a-a076-49a9-b182-d43bb1247288"
      },
      "outputs": [],
      "source": [
        "# Get the machine learning algorithm Naïve Bayes\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Fix the random seed for decision tree classifier\n",
        "np.random.seed(seed=0)   \n",
        "dt = DecisionTreeClassifier(max_depth=None)\n",
        "dt_model = dt.fit(X_train,y_train)\n",
        "print('Decision Tree accuracy for training set: %f' % dt_model.score(X_train, y_train))\n",
        "print('Decision Tree accuracy for test set: %f' % dt_model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1d46fa-3e2e-4e5a-a908-25b046f8331e",
      "metadata": {
        "id": "6e1d46fa-3e2e-4e5a-a908-25b046f8331e"
      },
      "source": [
        "After running these experiments you should have been able to get about 79% accuracy (on test set) for the nearest neighbor code and about 82% accuracy (on test set) using the Decision Tree algorithm.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CDmjJP10R4Kd",
      "metadata": {
        "id": "CDmjJP10R4Kd"
      },
      "source": [
        "That was easy! But it is important to know how to master these models yourself, so you're going to implement from scratch.  You'll practice this in the next sections."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c6f7ac-1bcc-4e8a-919c-73650f0225f2",
      "metadata": {
        "id": "13c6f7ac-1bcc-4e8a-919c-73650f0225f2"
      },
      "source": [
        "To ease the implementation in the next section, we'll first transform the data into `numpy` arrays and transform the labels into lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71c9452f-69c1-4d43-8d8f-729ffed05b7f",
      "metadata": {
        "id": "71c9452f-69c1-4d43-8d8f-729ffed05b7f"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
        "y_train, y_test = y_train.to_list(), y_test.to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7f64225-4087-4081-ac7d-641b4718be7d",
      "metadata": {
        "id": "c7f64225-4087-4081-ac7d-641b4718be7d"
      },
      "source": [
        "## 2. Programming : Implement $k$-Nearest Neighbor (kNN)\n",
        "\n",
        "We are going to implement a kNN to predict whether the wine is \"Good\" or \"Not good\" (so it is a binary or 2-class classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20606f9-5045-43b4-ac4f-7fd1c6829823",
      "metadata": {
        "id": "b20606f9-5045-43b4-ac4f-7fd1c6829823"
      },
      "source": [
        "Next we are going to build a list of helper functions to help build the kNN model. Your task is to implement these functions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b94665f-50da-4bad-9544-36845fbc8aa3",
      "metadata": {
        "id": "3b94665f-50da-4bad-9544-36845fbc8aa3"
      },
      "source": [
        "### .a Calculate Manhattan_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc1819f-fb15-4250-b763-b38cfa1c1221",
      "metadata": {
        "id": "2fc1819f-fb15-4250-b763-b38cfa1c1221"
      },
      "source": [
        "**Manhattan Distance**: Manhattan Distance measures the sum of the absolute differences of two vectors' Cartesian coordinates.\n",
        "\n",
        "$$\n",
        "L_1(a, b) = |a_1-b_1| + |a_2-b_2| + \\cdots + |a_n-b_n|\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9356fc39-5fb8-4b51-ad0c-e30c5c5b1aa1",
      "metadata": {
        "id": "9356fc39-5fb8-4b51-ad0c-e30c5c5b1aa1"
      },
      "source": [
        "**Your Turn (Question 1):** Complete the code below to calculate the Manhattan Distance between two data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c07a2c5-69f8-4f69-b1bb-205c6e1e5176",
      "metadata": {
        "id": "8c07a2c5-69f8-4f69-b1bb-205c6e1e5176"
      },
      "outputs": [],
      "source": [
        "def manhattan_distance(a, b):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      a (D) : a data point in numpy array of dimension D\n",
        "      b (D) : a data point in numpy array of dimension D\n",
        "    Returns:\n",
        "      dis(float): the Manhattan distance between the two data points\n",
        "    \"\"\"\n",
        "    dis = 0\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q2): Write your code here\n",
        "    # Hint: calculate vector c which is the element-wise absolute difference between vector a and b; find out the sum of all elements in c\n",
        "    # Hint: use the numpy library to speed up matrix and vector operations like difference, summation and square root. https://numpy.org/doc/stable/user/quickstart.html\n",
        "    #\n",
        "    ###########################\n",
        "    return dis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a522637b",
      "metadata": {
        "id": "a522637b"
      },
      "source": [
        "### .b Calculate Euclidean_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "377609cc",
      "metadata": {
        "id": "377609cc"
      },
      "source": [
        "**Euclidean Distance**: Euclidean Distance measures the length of the line segment bewteen two points in the Euclidean space.\n",
        "\n",
        "$$\n",
        "L_2(a, b) = \\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + \\cdots + (a_n-b_n)^2}\n",
        "$$\n",
        "\n",
        "Compared to the definition of Manhattan distance, it is natural to generalize the Euclidean Distance and Manhattan Distance into Minkowski Distance.\n",
        "\n",
        "**Minkowski Distance**: Minkowski Distance measures the p-order distance between two points. It is obvious that $p=1$ refers to Manhattan Distance while $p=2$ refers to Euclidean Distance.\n",
        "\n",
        "$$\n",
        "L_p(a, b) = (|a_1-b_1|^p + |a_2-b_2|^p + \\cdots + |a_n-b_n|^p)^{\\frac{1}{p}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b84f063",
      "metadata": {
        "id": "1b84f063"
      },
      "source": [
        "**Your Turn (Question 2):** Complete the code below to calculate the Minkowski Distance between two data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8655fcb1",
      "metadata": {
        "id": "8655fcb1"
      },
      "outputs": [],
      "source": [
        "def minkowski_distance(a, b, p):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      a (D) : a data point in numpy array of dimension D\n",
        "      b (D) : a data point in numpy array of dimension D\n",
        "      p     : distance order\n",
        "    Returns:\n",
        "      dis(float): the Minkowski distance between the two data points\n",
        "    \"\"\"\n",
        "    dis = 0\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q2): Write your code here\n",
        "    # Hint: calculate vector c which is the element-wise difference between vector a and b\n",
        "    # Hint: calculate p-norm of vector c; numpy.linalg.norm() is suggested to prevent data overflow; please refer to https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
        "\n",
        "    ###########################\n",
        "    return dis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HKrVZUZEAcTx",
      "metadata": {
        "id": "HKrVZUZEAcTx"
      },
      "source": [
        "**Testing**: test to see if the Minkowski distance is consistent with your manual calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9u-RXoP2Bx8v",
      "metadata": {
        "id": "9u-RXoP2Bx8v"
      },
      "outputs": [],
      "source": [
        "x1 = np.asarray([1, 2, 3])\n",
        "x2 = np.asarray([0, 0, 0])\n",
        "print('Minkowski Distance (p=1) between (1, 2, 3) and (0, 0, 0):', minkowski_distance(x1, x2, p=1))\n",
        "print('Minkowski Distance (p=2) between (1, 2, 3) and (0, 0, 0):', minkowski_distance(x1, x2, p=2))\n",
        "\n",
        "print('------------------------------------------------------------------')\n",
        "x1 = np.asarray([100, 20, 30])\n",
        "x2 = np.asarray([0, 0, 0])\n",
        "print('Minkowski Distance (p=1) between (100, 20, 30) and (0, 0, 0):', minkowski_distance(x1, x2, p=1))\n",
        "print('Minkowski Distance (p=2) between (100, 20, 30) and (0, 0, 0):', minkowski_distance(x1, x2, p=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sMP2yG7DT8Gn",
      "metadata": {
        "id": "sMP2yG7DT8Gn"
      },
      "source": [
        "From the tests above, we can see that the Minkowski Distance (p=2) is closer to the dimension in which the two vectors have the largest difference, *e.g.*, $(100-0)$, $(3-0)$ than Minkowski Distance (p=1). **Why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43359e84",
      "metadata": {
        "id": "43359e84"
      },
      "source": [
        "**Testing**: test to see if the Minkowski distance (p=1) is consistent with Manhattan distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9da0eba",
      "metadata": {
        "id": "b9da0eba"
      },
      "outputs": [],
      "source": [
        "x1 = np.asarray([1.5, 2.5, 3.5])\n",
        "x2 = np.asarray([0., 0., 0.])\n",
        "print('Manhattan Distance between (1.5, 2.5, 3.5) and (0., 0., 0.):', manhattan_distance(x1, x2))\n",
        "print('Minkowski Distance (p=1) between (1.5, 2.5, 3.5) and (0., 0., 0.):', minkowski_distance(x1, x2, p=1))\n",
        "\n",
        "x1 = np.asarray([10., 20., 30.])\n",
        "x2 = np.asarray([13., 4., 7.])\n",
        "print('Manhattan Distance between (10., 20., 30.) and (13., 4., 7.):', manhattan_distance(x1, x2))\n",
        "print('Minkowski Distance (p=1) between (10., 20., 30.) and (13., 4., 7.):', minkowski_distance(x1, x2, p=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15668366",
      "metadata": {
        "id": "15668366"
      },
      "source": [
        "Since we can use minkowski_distance to calculate both Manhattan Distance and Euclidean Distance, we will keep using minkowski_distance function instead of manhattan_distance in the following programming.\n",
        "\n",
        "**Optional**: test to see what happened when p > 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CmfD_TS4A6a3",
      "metadata": {
        "id": "CmfD_TS4A6a3"
      },
      "outputs": [],
      "source": [
        "# Testing: This should return 84.44345780422779\n",
        "print(minkowski_distance(X_test[0], X_train[0], p=2))\n",
        "\n",
        "# Testing: This should return 99.69812000000002\n",
        "print(minkowski_distance(X_test[0], X_train[0], p=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec1e5566-f7bd-4ba7-9882-932fc27e26d7",
      "metadata": {
        "id": "ec1e5566-f7bd-4ba7-9882-932fc27e26d7"
      },
      "source": [
        "### .c Find k Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13eea814-4b5c-4437-bd63-27b508fcc89e",
      "metadata": {
        "id": "13eea814-4b5c-4437-bd63-27b508fcc89e"
      },
      "source": [
        "**$k$-NN function**: The $k$-NN function will find the k nearest data points given an array of distances. We want to return the corresponding labels of the k Nearest Neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d41ac3ab-2523-4992-9f29-fa8e6b18610c",
      "metadata": {
        "id": "d41ac3ab-2523-4992-9f29-fa8e6b18610c"
      },
      "source": [
        "**Your Turn (Question 3):** Complete the code below to find out the kNN's labels with the given array of distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "603a33b7-431f-4b37-b685-08e5556e578b",
      "metadata": {
        "id": "603a33b7-431f-4b37-b685-08e5556e578b"
      },
      "outputs": [],
      "source": [
        "def find_kNN_labels(distances, labels, k):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      distances (m,) : a numpy array of dimension m that contains the distances between the test data point and all training data points \n",
        "      labels (m,) : a list of length m that contains the labels of all training data points\n",
        "      k: the number of nearest neighbors\n",
        "    Returns:\n",
        "      knn_labels (k,): the labels of the k nearest neighbors\n",
        "    \"\"\"\n",
        "    \n",
        "    knn_labels = []\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q3): Write your code here\n",
        "    # Hint: use numpy.argsort function to sort distances array and obtain the sorted indice; select the k indice with the least distances; return the corresponding labels of the selected k indice\n",
        "    # Hint: use \"slicing\", e.g., list_of_names[:k], to select the first k elements in python list and numpy array. https://stackoverflow.com/questions/509211/understanding-slice-notation\n",
        "    \n",
        "    ###########################\n",
        "    return knn_labels\n",
        "\n",
        "# Testing: This should return [2, 1, 0, 1, 2]\n",
        "sample_distances = [1.97, 1.94, 0.16, 0.91, 2.05, 1.5, 1.86, 2.01, 1.9, 1.67, 1.9, 1.14, 2.1, 1.94, 2.68, 0.08, 3.98, 3.05, 1.59, 2.4]\n",
        "sample_labels = [0, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 2, 1, 2, 0, 2, 2, 1]\n",
        "print(find_kNN_labels(sample_distances, sample_labels, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2FbdUK94lZpp",
      "metadata": {
        "id": "2FbdUK94lZpp"
      },
      "source": [
        "### .d Find the Majority Class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1P6SRD1Mmudn",
      "metadata": {
        "id": "1P6SRD1Mmudn"
      },
      "source": [
        "**Majority Class Function**: When finding the $k$-Nearest Neighbors, we return the value that represents the majority of the $k$ instances of the class as the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cs6Px7tkl-ud",
      "metadata": {
        "id": "Cs6Px7tkl-ud"
      },
      "source": [
        "**Your Turn (Question 4):** Complete the code below to find the majority class from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ri7bAZaQlXTV",
      "metadata": {
        "id": "ri7bAZaQlXTV"
      },
      "outputs": [],
      "source": [
        "def get_majority_class(labels):\n",
        "    \"\"\" \n",
        "    Args:\n",
        "      labels(m): The corresponding labels of current sub-dataset\n",
        "          m: num_rows\n",
        "    Returns:\n",
        "      major: Type:String. The major class of this sub-dataset(e.g \"Good Wine\" or \"Not Good\")\n",
        "    \"\"\"\n",
        "    major = \"\"\n",
        "    \n",
        "    # freq will store the number of occurences of the target labels\n",
        "    freq = {}\n",
        "    for entry in labels:\n",
        "        if (entry in freq):\n",
        "            freq[entry] += 1.0\n",
        "        else:\n",
        "            freq[entry] = 1.0\n",
        "            \n",
        "    major = \"\"\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q4): Write your code here\n",
        "    # Hint: Loop through each entry in labels, then find which entry occurs most frequently.\n",
        "    #\n",
        "    \n",
        "    ###########################\n",
        "    return major\n",
        "  \n",
        "\n",
        "# Testing: This should return 'Good'\n",
        "sample_y = y_train[50:56]\n",
        "print(get_majority_class(sample_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "366e998f-51cf-43a9-a33a-bec29c01913a",
      "metadata": {
        "id": "366e998f-51cf-43a9-a33a-bec29c01913a"
      },
      "source": [
        "### .e Run $k$-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69143bc8-f0ba-41b5-ae1f-341746ef9910",
      "metadata": {
        "id": "69143bc8-f0ba-41b5-ae1f-341746ef9910"
      },
      "source": [
        "The following code `run_knn` is provided to you to run your kNN helper functions. \n",
        "\n",
        "You do not have to understand its code for the purpose of this exercise. Just run it and check the accuracy. \n",
        "\n",
        "Note that because the prediction of a single data points requires traversing the whole training set. The code is a bit slow. It can take $2\\sim 4$ minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc244881-0caa-41c1-a237-e6ebf10e93ff",
      "metadata": {
        "id": "bc244881-0caa-41c1-a237-e6ebf10e93ff"
      },
      "outputs": [],
      "source": [
        "def run_knn(distance_metric_function, **kwags):\n",
        "    k = 1\n",
        "    correct = 0\n",
        "    for test_entry, label in zip(X_test, y_test):\n",
        "\n",
        "        ## find out the distance between the test data point and all training data points\n",
        "        distances = []\n",
        "        for train_entry in X_train:\n",
        "            distances.append(distance_metric_function(test_entry, train_entry, **kwargs))\n",
        "\n",
        "        knn_labels = find_kNN_labels(distances, y_train, k)\n",
        "        prediction = get_majority_class(knn_labels)\n",
        "        \n",
        "        if prediction == label:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / len(y_test)\n",
        "    print('Final accuracy')\n",
        "    print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cbc9fcf-ee0b-4654-b3a6-4cc41021130f",
      "metadata": {
        "id": "8cbc9fcf-ee0b-4654-b3a6-4cc41021130f"
      },
      "source": [
        "The following code runs $k$-NN with the Euclidean distance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a069a328-adb9-42ea-bd3b-3499ea0a2132",
      "metadata": {
        "id": "a069a328-adb9-42ea-bd3b-3499ea0a2132"
      },
      "outputs": [],
      "source": [
        "## The accuracy should be 0.7945578231292517.\n",
        "kwargs = {}\n",
        "kwargs['p'] = 2\n",
        "run_knn(minkowski_distance, **kwargs) # note that minkowski_distance is a function, not a value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3a2c83e-95a2-42a4-933f-311d524a80ac",
      "metadata": {
        "id": "f3a2c83e-95a2-42a4-933f-311d524a80ac"
      },
      "source": [
        "The following code runs $k$-NN with the Manhattan distance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d81df58",
      "metadata": {
        "id": "9d81df58"
      },
      "outputs": [],
      "source": [
        "## The accuracy should be 0.8020408163265306\n",
        "kwargs = {}\n",
        "kwargs['p'] = 1\n",
        "run_knn(minkowski_distance, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae64e6d7",
      "metadata": {
        "id": "ae64e6d7"
      },
      "source": [
        "**Optional**: The following code runs $k$-NN with the Minkowski distance (p=10). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "858f4d42-9a0c-448b-8440-b8c3c8927853",
      "metadata": {
        "id": "858f4d42-9a0c-448b-8440-b8c3c8927853"
      },
      "outputs": [],
      "source": [
        "## The accuracy should be 0.7925170068027211\n",
        "kwargs = {}\n",
        "kwargs['p'] = 10\n",
        "run_knn(minkowski_distance, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0048bf28-85ef-4f17-91df-e5bd34fd7ab5",
      "metadata": {
        "id": "0048bf28-85ef-4f17-91df-e5bd34fd7ab5"
      },
      "source": [
        "## 3. Programming : Implement Decision Tree\n",
        "\n",
        "We are going to implement a decision tree to predict whether the wine is \"Good\" or \"Not good\" (again, it is a 2-class classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f497ee5d-968f-4554-b9d7-e1307c990ec3",
      "metadata": {
        "id": "f497ee5d-968f-4554-b9d7-e1307c990ec3"
      },
      "source": [
        "### .a Discretize the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cee51a3-b13f-4f64-a479-2202cbea65b8",
      "metadata": {
        "id": "5cee51a3-b13f-4f64-a479-2202cbea65b8"
      },
      "source": [
        "We are going to make use of 11 _attributes_ (also known as _features_ or _input dimensions_, but for this decision trees to be consistent with the algorithm, we'll use \"attributes\"). \n",
        "\n",
        "In this assignment, we will implement a basic version of Decision tree that takes in categorical attributes. Thus, we are going to discretize the continuous features, where values **larger** than the mean is assigned **1** and values **smaller** than the mean is assigned **0**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c8a15a-091e-40e7-b85f-61a8e79e052e",
      "metadata": {
        "id": "52c8a15a-091e-40e7-b85f-61a8e79e052e"
      },
      "outputs": [],
      "source": [
        "def discretize_data(train_data, test_data):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        train_data (np array): the training set\n",
        "        test_data (np array): the test set\n",
        "    \n",
        "    Returns:\n",
        "    np_train_data (np array): contains the discretized training set\n",
        "    np_test_data (np array): contains the discretized test set\n",
        "    \"\"\"\n",
        "    train_data_discrete = np.zeros_like(train_data) # initialize\n",
        "    test_data_discrete = np.zeros_like(test_data)\n",
        "    \n",
        "    D = train_data_discrete.shape[1]\n",
        "    \n",
        "    for i in range(D):\n",
        "        mean_value = np.mean(train_data[:, i]) # get mean values at which to label as 1 (larger) or 0 (smaller)\n",
        "        train_data_discrete[:, i] = (train_data[:, i] > mean_value).astype(float)\n",
        "        test_data_discrete[:, i] = (test_data[:, i] > mean_value).astype(float)\n",
        "    \n",
        "    return train_data_discrete, test_data_discrete\n",
        "\n",
        "X_train_discrete, X_test_discrete = discretize_data(X_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06203550-afe2-4496-b0b8-53da94f6b5c1",
      "metadata": {
        "id": "06203550-afe2-4496-b0b8-53da94f6b5c1"
      },
      "outputs": [],
      "source": [
        "sample_X = X_train_discrete[10:20]\n",
        "sample_y = y_train[10:20]\n",
        "sample_X, sample_y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193e7af2-57e0-494a-b1fe-ce16348f05ea",
      "metadata": {
        "id": "193e7af2-57e0-494a-b1fe-ce16348f05ea"
      },
      "source": [
        "### .b Choose the best Feature based on Majority"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08aa95d2-a41a-4288-9cbc-f5fd38afccc1",
      "metadata": {
        "id": "08aa95d2-a41a-4288-9cbc-f5fd38afccc1"
      },
      "source": [
        "**Majority Class Function**: When you reach a terminal node (leaf) in a decision tree, we return the label that represents the majority of the sub-dataset's instances of the class as the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bv2g3fB_mCQS",
      "metadata": {
        "id": "Bv2g3fB_mCQS"
      },
      "source": [
        "We can reuse the `get_majority_class` function from the implementation of $k$-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "732bc81a-6c81-4d82-94a1-f64ac5ca4eed",
      "metadata": {
        "id": "732bc81a-6c81-4d82-94a1-f64ac5ca4eed"
      },
      "source": [
        "### .c Calculate Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd7f40c-8263-429c-b2ad-71d0d14dc61f",
      "metadata": {
        "id": "5bd7f40c-8263-429c-b2ad-71d0d14dc61f"
      },
      "source": [
        "**Entropy Function**: Calculate the entropy of this current sub-dataset:\n",
        "\n",
        "Recall: $H(X) = - \\sum_{i=0}^C p_i\\log(p_i)$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f95fbba7-c7eb-4a60-a123-38f109edb54b",
      "metadata": {
        "id": "f95fbba7-c7eb-4a60-a123-38f109edb54b"
      },
      "source": [
        "**Your Turn (Question 5):** Complete the code below to calculate entropy of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3426b56e-ab12-416f-b9b0-dd91099d828e",
      "metadata": {
        "id": "3426b56e-ab12-416f-b9b0-dd91099d828e"
      },
      "outputs": [],
      "source": [
        "def entropy(labels):\n",
        "    \"\"\" \n",
        "    Args:\n",
        "      labels(m): The corresponding labels of current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "    Returns:\n",
        "      dataEntropy: The entropy of this current sub-dataset\n",
        "    \"\"\"\n",
        "    dataEntropy = 0.0\n",
        "    # freq will store the number of occurrences of the target labels\n",
        "    freq = {}\n",
        "    for entry in labels:\n",
        "        if (entry in freq):\n",
        "            freq[entry] += 1.0\n",
        "        else:\n",
        "            freq[entry] = 1.0\n",
        "\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q5): Write your code here\n",
        "    # Hint: Loop through each row of data, get the last column, find number of occurrences of each value, then use the above equation.\n",
        "    #\n",
        "    \n",
        "    ###########################\n",
        "    \n",
        "    return dataEntropy\n",
        "\n",
        "# Testing: This should return 0.9709505944546686 (in log 2)\n",
        "print(entropy(sample_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43dcec20-82f1-4b7b-9500-313b9bbe8bc2",
      "metadata": {
        "id": "43dcec20-82f1-4b7b-9500-313b9bbe8bc2"
      },
      "source": [
        "### .d Calculate Information Gain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3c82aa8-3f79-4b57-a111-4ef1969eee7b",
      "metadata": {
        "id": "c3c82aa8-3f79-4b57-a111-4ef1969eee7b"
      },
      "source": [
        "**Information Gain**: The information gained by splitting the current (sub)-dataset using the attribute.\n",
        "\n",
        "Recall:  Information Gain is a metric that measures the expected reduction in the impurity of the collection $S$, caused by splitting the data according to any given attribute. A chosen attribute $x_i$ divides the example set S into subsets\n",
        "$S_1 , S_2 , ... , S_{C_i}$ according to the $C_i$ distinct values for $x_i$ .\n",
        "The entropy then reduces to the entropy of the subsets $S_1 , S_2 , ... , S_{C_i}$:\n",
        "\n",
        "<div align=\"center\">\n",
        "$\\text{remainder}(S, x_i) = \\sum_{j=1}^{C_i} \\frac{|S_j|}{|S|} H(S_j)$\n",
        "</div>\n",
        "\n",
        "The Information Gain (IG; “reduction in entropy”) from knowing the value of $x_i$ is:\n",
        "<div align=\"center\">\n",
        "$IG(S, x_i) = H(S) - \\text{remainder}(S, x_i) $  \n",
        "</div>\n",
        "\n",
        "Subsequently, we choose the attribute with the largest IG."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8541937a-d959-4e6d-84bc-9610b83daa66",
      "metadata": {
        "id": "8541937a-d959-4e6d-84bc-9610b83daa66"
      },
      "source": [
        "**Your Turn (Question 6):** Complete the code below to calculate information gain of a given attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de08aa3c-16fa-498f-8b16-57d571126ad3",
      "metadata": {
        "id": "de08aa3c-16fa-498f-8b16-57d571126ad3"
      },
      "outputs": [],
      "source": [
        "def info_gain(data, labels, attribute, attributes):\n",
        "    \"\"\" \n",
        "    Args:\n",
        "      data(m, D): The current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "          D: num_features\n",
        "      labels(m): The corresponding labels of current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "      attribute: The attribute used to split data\n",
        "      attributes: The list of current remaining attributes\n",
        "    Returns:\n",
        "      info_gain : information gain of the given dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    freq = {}\n",
        "    subsetEntropy = 0.0\n",
        "    \n",
        "    # Get the column index of this attribute\n",
        "    i = attributes.index(attribute)\n",
        "    \n",
        "    for entry in data:\n",
        "        if (entry[i] in freq):\n",
        "            freq[entry[i]] += 1.0\n",
        "        else:\n",
        "            freq[entry[i]]  = 1.0\n",
        "\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q6): Write your code here\n",
        "    # Hint: Split the data based on the value at index i. Find the subsetEntropy of\n",
        "    # each sub-dataset, then use the formula of Information Gain to calculate subsetEntropy.\n",
        "    #\n",
        "\n",
        "    ###########################\n",
        "    return (entropy(labels) - subsetEntropy)\n",
        "\n",
        "# Testing: This should return 0.01997309402197489\n",
        "attributes = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
        "print(info_gain(sample_X, sample_y, 'residual sugar', attributes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49490e04-d33c-4f72-a6d4-dadcf2dcaf5b",
      "metadata": {
        "id": "49490e04-d33c-4f72-a6d4-dadcf2dcaf5b"
      },
      "source": [
        "### .e Find the Best Attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156d86b5-6fa4-43a1-a7e2-22c74a98d418",
      "metadata": {
        "id": "156d86b5-6fa4-43a1-a7e2-22c74a98d418"
      },
      "source": [
        "Now we will write a function to choose the **best** (most discriminating) attribute for a given data, here best indicates having **largest** information gain (IG) among all attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b03b21-a83c-4b15-9eec-261c552af288",
      "metadata": {
        "id": "f9b03b21-a83c-4b15-9eec-261c552af288"
      },
      "source": [
        "**Your Turn (Question 7):** Complete the code below to get the best attribute based on information gain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb2edb1-21b2-44bd-b3c8-cc700bfb6e8a",
      "metadata": {
        "id": "5fb2edb1-21b2-44bd-b3c8-cc700bfb6e8a"
      },
      "outputs": [],
      "source": [
        "def get_best_gain_attribute(data, labels, attributes):\n",
        "    \"\"\" \n",
        "    Args:\n",
        "      data(m, D): The current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "          D: num_features + 1 (last column is the label)\n",
        "      attributes: The list of current remaining attributes\n",
        "    Returns:\n",
        "      best: The best attribute to split based on info gain.\n",
        "    \"\"\"\n",
        "    best = attributes[0]\n",
        "    \n",
        "    max_gain = 0\n",
        "    for attr in attributes:\n",
        "      ###########################\n",
        "      #\n",
        "      # Your Turn (Q7): Write your code here\n",
        "      # Hint: For each attribute in attributes, use info_gain function above\n",
        "      # to know which attribute is the best option\n",
        "      #\n",
        "      pass\n",
        "      ###########################\n",
        "    return best\n",
        "  \n",
        "# Testing: This should return 'citric acid'\n",
        "attributes = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
        "print(get_best_gain_attribute(sample_X, sample_y, attributes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6aab711-569d-4ace-849c-0684a1e420f7",
      "metadata": {
        "id": "b6aab711-569d-4ace-849c-0684a1e420f7"
      },
      "source": [
        "### .f Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f15bcceb-dbe0-4f2e-99dc-c1da03572196",
      "metadata": {
        "id": "f15bcceb-dbe0-4f2e-99dc-c1da03572196"
      },
      "source": [
        "We will define two helper functions here. First, `get_unique_values` returns the unique values of a given attribute. The second function, `get_sub_data` returns the subset of rows (instances) containing a specific value of a chosen attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc3c4a5-35ce-4c0b-893c-08171dd8ffe5",
      "metadata": {
        "id": "2fc3c4a5-35ce-4c0b-893c-08171dd8ffe5"
      },
      "outputs": [],
      "source": [
        "# These two functions are helper functions\n",
        "# This function will get unique values for that particular attribute from the given data\n",
        "def get_unique_values(data, attributes, attribute):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      data (m,D) : Current subset of data\n",
        "      attributes : The list of current remaining attributes\n",
        "      attribute : Our point of interest\n",
        "    \n",
        "    Returns:\n",
        "      values : List of unique values for our point of interest\n",
        "    \"\"\"\n",
        "    index = attributes.index(attribute)\n",
        "    values = []\n",
        "    # \n",
        "    for entry in data:\n",
        "        if entry[index] not in values:\n",
        "            values.append(entry[index])\n",
        "\n",
        "    return values\n",
        "\n",
        "# This function will get all the rows of the data where the chosen \"best\" attribute has a value \"val\"\n",
        "def get_sub_data(data, labels, attributes, best, val):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      data (m,D) : Current subset of data\n",
        "      labels (m) : Corresponding labels of current subset of data\n",
        "      attributes : The list of current remaining attributes\n",
        "      best : The attribute of which data we will extract\n",
        "      val : We are interested only on this value of the `best` attribute\n",
        "    Returns:\n",
        "      new_data : Data subset containing only those rows where `best` attribute = val\n",
        "      new_labels : Label subset containing only those values where `best` attribute = val\n",
        "    \"\"\"\n",
        "    new_data = [[]]\n",
        "    new_labels = []\n",
        "    attribute_index = attributes.index(best)\n",
        "\n",
        "    for index, entry in enumerate(data):\n",
        "        if (entry[attribute_index] == val):\n",
        "            newEntry = []\n",
        "            for i in range(0,len(entry)):\n",
        "                if(i != attribute_index):\n",
        "                    newEntry.append(entry[i])\n",
        "            new_data.append(newEntry)\n",
        "            new_labels.append(labels[index])\n",
        "    new_data.remove([])    \n",
        "    return new_data, new_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34bea7f2-dc88-4ef9-ada8-c0225dd3fa00",
      "metadata": {
        "id": "34bea7f2-dc88-4ef9-ada8-c0225dd3fa00"
      },
      "source": [
        "### .g Build the Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a716217b-4105-4fb4-ab5f-40666d1d671e",
      "metadata": {
        "id": "a716217b-4105-4fb4-ab5f-40666d1d671e"
      },
      "source": [
        "In the below code, your task is to build a tree recursively. Starting at the root, pick the best attribute to split on, and call the `build_tree` function on each of the sub-trees.  Check the inlined code comments for clarification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44185b20-9ad4-4afd-8a83-b63bd96d6e4a",
      "metadata": {
        "id": "44185b20-9ad4-4afd-8a83-b63bd96d6e4a"
      },
      "source": [
        "**Your Turn (Question 8):** Complete the code below to build the tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5327d2e-0c60-41a1-a9f0-5c9870e50973",
      "metadata": {
        "id": "c5327d2e-0c60-41a1-a9f0-5c9870e50973"
      },
      "outputs": [],
      "source": [
        "def build_tree(data, labels, attributes, default):\n",
        "    \"\"\" \n",
        "      Args:\n",
        "        data(m, D): The current sub-dataset of the sub-tree\n",
        "            m: num_rows\n",
        "            D: num_features + 1 (last column is the label)\n",
        "        attributes: The list of current remaining attributes\n",
        "      Returns:\n",
        "        tree: The constructed tree as object. For example if the root is gender,\n",
        "              then a tree of depth 2 is like \n",
        "              {'gender': {'male': sub_tree1, 'female': sub_tree2}}\n",
        "    \"\"\"\n",
        "    data = data[:]\n",
        "    tree = {}\n",
        "    ##################################################################\n",
        "    ## Your Turn (Q8): Finish the implementation of the below code ##\n",
        "\n",
        "    ###########################\n",
        "    return tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac247c4f-5622-45da-8fb4-b48f1b9e95d2",
      "metadata": {
        "id": "ac247c4f-5622-45da-8fb4-b48f1b9e95d2"
      },
      "source": [
        "The below code block containing `run_decision_tree` does exactly as its name, and you don't have to understand its code for the purpose of this exercise. Just run it and check the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16a807fb-aebf-4b14-8d49-3b629878e847",
      "metadata": {
        "id": "16a807fb-aebf-4b14-8d49-3b629878e847"
      },
      "outputs": [],
      "source": [
        "# Class Node which will be used while classifying a test instance using the tree built (\"fit\") earlier\n",
        "class Node():\n",
        "    value = \"\"\n",
        "    children = []\n",
        "\n",
        "    def __init__(self, val, dictionary):\n",
        "        self.value = val\n",
        "        if (isinstance(dictionary, dict)):\n",
        "            self.children = list(dictionary.keys())\n",
        "\n",
        "def run_decision_tree():\n",
        "    attributes = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
        "    tree = build_tree(X_train_discrete, y_train, attributes, get_majority_class(y_train))\n",
        "    results = []\n",
        "\n",
        "    for entry, label in zip(X_test_discrete, y_test):\n",
        "        tempDict = tree.copy()\n",
        "        result = \"\"\n",
        "        while(isinstance(tempDict, dict)):\n",
        "            root = Node(list(tempDict.keys())[0], tempDict[list(tempDict.keys())[0]])\n",
        "            tempDict = tempDict[list(tempDict.keys())[0]]\n",
        "            index = attributes.index(root.value)\n",
        "            value = entry[index]\n",
        "            if(value in list(tempDict.keys())):\n",
        "                child = Node(value, tempDict[value])\n",
        "                result = tempDict[value]\n",
        "                tempDict = tempDict[value]\n",
        "            else:\n",
        "                result = \"Null\"\n",
        "                break\n",
        "        if result != \"Null\":\n",
        "            results.append(result == label)\n",
        "        \n",
        "    accuracy = float(results.count(True))/float(len(results))\n",
        "#     print(results)\n",
        "    print(\"FINAL ACCURACY: \")\n",
        "    print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25076d80-85cc-47bb-b63f-c8956de81e4b",
      "metadata": {
        "id": "25076d80-85cc-47bb-b63f-c8956de81e4b"
      },
      "outputs": [],
      "source": [
        "run_decision_tree()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e574952-8958-4b4a-b1d9-65233d061840",
      "metadata": {
        "id": "4e574952-8958-4b4a-b1d9-65233d061840"
      },
      "source": [
        "## 4. Comparison between $k$-NN and Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FQ85usDdFUbI",
      "metadata": {
        "id": "FQ85usDdFUbI"
      },
      "source": [
        "The above is akin to a skeleton of a machine learning project.  Critical for understanding is knowing why the performance of a learner is as it is. As a scientist, you'll want to know why you think a learner performs well or poorly and **then** use experiments to verify your hunches.  To build up this skill we need to practice it, as you'll need to apply this in your own group projects later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b610577-3690-403c-b18f-93214458bda2",
      "metadata": {
        "id": "7b610577-3690-403c-b18f-93214458bda2"
      },
      "source": [
        "**Your Turn (Question 9):** Compare the performances of our implementation of $k$-NN and decision tree. Tell us what you have observed and why one of the models performs better than the other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gBNkBYnOo3ks",
      "metadata": {
        "id": "gBNkBYnOo3ks"
      },
      "source": [
        "**Your answer here (Question 9)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VGlAWahG2MW9",
      "metadata": {
        "id": "VGlAWahG2MW9"
      },
      "source": [
        "**Your Turn (Question 10):** Compare the running times of $k$-NN and decision tree. Briefly tell us why $k$-NN is much slower than the other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gYlhuN-vpBTy",
      "metadata": {
        "id": "gYlhuN-vpBTy"
      },
      "source": [
        "**Your answer here (Question 10)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fSVen-P92HPP",
      "metadata": {
        "id": "fSVen-P92HPP"
      },
      "source": [
        "**Your Turn (Question 11):** Compare the performances of your implementation of decision tree and sklearn's implementation. Which one has the better performance? Why do you think that is?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5W8zvwKapCvJ",
      "metadata": {
        "id": "5W8zvwKapCvJ"
      },
      "source": [
        "**Your answer here (Question 11)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h0ySjsCG3r9j",
      "metadata": {
        "id": "h0ySjsCG3r9j"
      },
      "source": [
        "**Your Turn (Question 12):** Identify an instance where $k$-NN predicts a different label than Decision Tree. Briefly tell us why you think they predicted differently, and what can you do to make them agree more?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z3c_dfKQ3sAy",
      "metadata": {
        "id": "Z3c_dfKQ3sAy"
      },
      "source": [
        "**Your answer here (Question 12)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j5yziGEN4J1U",
      "metadata": {
        "id": "j5yziGEN4J1U"
      },
      "source": [
        "Congratulations 🎉 🎉 ! You have come to the end of the assignment. \n",
        "**Remember** to submit your \"Your Turn\" code, results, and answers in LumiNUS Quiz to be graded for your work."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}